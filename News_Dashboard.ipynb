{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9mzJACnDBaP4YHAerjbw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bayhaqy/News-Dashboard/blob/main/News_Dashboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# News-Dashboard\n",
        "\n",
        "https://huggingface.co/spaces/Bayhaqy/News-Dashboard"
      ],
      "metadata": {
        "id": "gPiEKFObG_wK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Repository"
      ],
      "metadata": {
        "id": "eHci2h7eVT07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bayhaqy/News-Dashboard.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fK1jJajVX1-",
        "outputId": "7fa905c4-da17-431f-97c9-d99dd8351fae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'News-Dashboard'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 28 (delta 9), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (28/28), 16.91 KiB | 8.46 MiB/s, done.\n",
            "Resolving deltas: 100% (9/9), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd News-Dashboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOKW1gMcVaXf",
        "outputId": "3ab01d55-e1df-4bd8-f727-f4e9ef4a9fdc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/News-Dashboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUih2i6pVtXE",
        "outputId": "40f6aaf6-5cd0-4e28-9512-035c7f335534"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py  LICENSE  \u001b[0m\u001b[01;34mpages\u001b[0m/  README.md  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### apps.py"
      ],
      "metadata": {
        "id": "dRF_M8riAQWj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYzO4qfa_22M",
        "outputId": "39cfa2c5-b1f4-4f5c-bc46-1f0b2d0aa032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# News Information and data article\n",
        "from newspaper import Article, Config\n",
        "from gnews import GNews\n",
        "\n",
        "# Data Analysis and Profiling\n",
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport\n",
        "from st_aggrid import AgGrid\n",
        "\n",
        "# Streamlit for Building the Dashboard\n",
        "import streamlit as st\n",
        "from streamlit_pandas_profiling import st_profile_report\n",
        "\n",
        "# Language Detection\n",
        "from langdetect import detect\n",
        "\n",
        "# NLP and Text Processing\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from deep_translator import GoogleTranslator\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Sentiment Analysis\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# URL Parsing\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Data Visualization\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Word Cloud Generation\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Other Libraries\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "import logging\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "\n",
        "# NLTK Data Download\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "## ............................................... ##\n",
        "# Set page configuration (Call this once and make changes as needed)\n",
        "st.set_page_config(page_title='News Scrapping',  layout='wide', page_icon=':newspaper:')\n",
        "\n",
        "with st.container():\n",
        "    # Initialize Streamlit app\n",
        "    st.title('News Article Scrapping')\n",
        "    st.write(\"Created by Bayhaqy\")\n",
        "\n",
        "## ............................................... ##\n",
        "# Set up logging\n",
        "logging.basicConfig(filename='news_processing.log', level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "## ............................................... ##\n",
        "# Function for get model and tokenize\n",
        "@st.cache_resource\n",
        "def get_models_and_tokenizers():\n",
        "    model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "    #model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function for sentiment analysis\n",
        "@st.cache_resource\n",
        "def analyze_sentiment_distilbert(text, _model, _tokenizer):\n",
        "    try:\n",
        "        tokens_info = _tokenizer(text, truncation=True, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            raw_predictions = _model(**tokens_info).logits\n",
        "\n",
        "        predicted_class_id = raw_predictions.argmax().item()\n",
        "        predict = _model.config.id2label[predicted_class_id]\n",
        "\n",
        "        softmaxed = int(torch.nn.functional.softmax(raw_predictions[0], dim=0)[1] * 100)\n",
        "        if (softmaxed > 70):\n",
        "            status = 'Not trust'\n",
        "        elif (softmaxed > 40):\n",
        "            status = 'Not sure'\n",
        "        else:\n",
        "            status = 'Trust'\n",
        "        return status, predict\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Sentiment analysis error: {str(e)}\")\n",
        "        return 'N/A', 'N/A'\n",
        "\n",
        "# Function for sentiment analysis using VADER\n",
        "@st.cache_data\n",
        "def analyze_sentiment_vader(text):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    compound_score = sentiment['compound']\n",
        "    if compound_score >= 0.05:\n",
        "        return 'Positive'\n",
        "    elif compound_score <= -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "# Function for sentiment analysis using TextBlob\n",
        "@st.cache_data\n",
        "def analyze_sentiment_textblob(text):\n",
        "    analysis = TextBlob(text)\n",
        "    polarity = analysis.sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return 'Positive'\n",
        "    elif polarity < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "## ............................................... ##\n",
        "# Function to process an article\n",
        "@st.cache_data\n",
        "def process_article(url, _config):\n",
        "    try:\n",
        "        article = Article(url=url, config=_config)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        # Check if publish_date is not None before further processing\n",
        "        if article.publish_date is None:\n",
        "            return None  # Skip processing and return None\n",
        "\n",
        "        # Check if text is not None before further processing\n",
        "        if len(article.text) <= 5:\n",
        "            return None  # Skip processing and return None\n",
        "\n",
        "        # Get the article data if publish_date is not not None\n",
        "        text = article.text\n",
        "        url = article.canonical_link\n",
        "        source_url = urlparse(url).netloc\n",
        "\n",
        "        title = article.title\n",
        "        authors = article.authors\n",
        "        #publish_date = article.publish_date.strftime('%Y-%m-%d %H:%M:%S%z')\n",
        "        publish_date = article.publish_date.strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "        article.nlp()\n",
        "        keywords = article.meta_keywords\n",
        "        summary = article.summary\n",
        "\n",
        "        language = detect(title)\n",
        "\n",
        "        return publish_date, language, url, source_url, title, authors, keywords, text, summary\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Article processing error: {str(e)}\")\n",
        "        return None  # Skip processing and return None\n",
        "\n",
        "# Function for translation\n",
        "@st.cache_data\n",
        "def translate_text(text, source='auto', target='en'):\n",
        "    try:\n",
        "        if source != target:\n",
        "            text = GoogleTranslator(source=source, target=target).translate(text)\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Translation error: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "## ............................................... ##\n",
        "# Function to preprocess the data\n",
        "@st.cache_data\n",
        "def preprocessing_data(df):\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset='Translation')\n",
        "\n",
        "    # Reset the index to add the date column\n",
        "    df.reset_index(inplace=True,drop=True)\n",
        "\n",
        "    # Function to clean and preprocess text\n",
        "    def clean_text(text):\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove non-alphanumeric characters\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "        # Tokenize text\n",
        "        words = nltk.word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "\n",
        "        # Lemmatize words\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    # Apply the clean_text function to the \"Translation\" column\n",
        "    df['Cleaned Translation'] = df['Translation'].apply(clean_text)\n",
        "\n",
        "    return df\n",
        "\n",
        "## ............................................... ##\n",
        "# Function to create a Word Cloud\n",
        "@st.cache_data\n",
        "def create_wordcloud(df):\n",
        "    # Combine all text\n",
        "    text = ' '.join(df['Cleaned Translation'])\n",
        "\n",
        "    # Create a Word Cloud\n",
        "    wordcloud = WordCloud(width=700, height=400, max_words=80).generate(text)\n",
        "\n",
        "    # Convert the word cloud to an image\n",
        "    wordcloud_image = wordcloud.to_image()\n",
        "\n",
        "    # Display the Word Cloud using st.image\n",
        "    st.image(wordcloud_image, use_column_width=True)\n",
        "\n",
        "## ............................................... ##\n",
        "with st.container():\n",
        "    # Input search parameters\n",
        "    search_term = st.text_input('Enter a search term :', 'Indonesia')\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    with col1:\n",
        "        period = st.text_input('Enter a news period :', '7d')\n",
        "        max_results = st.number_input('Maximum number of results :', min_value=1, value=10)\n",
        "    with col2:\n",
        "        country = st.text_input('Country :', 'Indonesia')\n",
        "        language = st.text_input('Language :', 'indonesian')\n",
        "    with col3:\n",
        "        start_date = st.date_input('Start Date :', pd.to_datetime('2023-01-01'))\n",
        "        end_date = st.date_input('End Date :', pd.to_datetime('2023-12-01'))\n",
        "\n",
        "## ............................................... ##\n",
        "with st.container():\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    with col1:\n",
        "        # Checkbox options for different processing steps\n",
        "        include_translation = st.checkbox(\"Include Translation\", value=True)\n",
        "        include_sentiment_analysis = st.checkbox(\"Include Sentiment Analysis\", value=True)\n",
        "    with col2:\n",
        "        include_sentiment_vader = st.checkbox(\"Include VADER Sentiment Analysis\", value=True)\n",
        "        include_sentiment_textblob = st.checkbox(\"Include TextBlob Sentiment Analysis\", value=True)\n",
        "\n",
        "## ............................................... ##\n",
        "# Create a variable to track whether the data has been processed\n",
        "data_processed = False\n",
        "\n",
        "## ............................................... ##\n",
        "# Create a custom configuration\n",
        "config = Config()\n",
        "config.number_threads = 500\n",
        "config.request_timeout = 10\n",
        "\n",
        "## ............................................... ##\n",
        "# Initialize the DataFrame\n",
        "df = pd.DataFrame(columns=['Publish_Date', 'Language', 'URL', 'Source_Url', 'Title', 'Authors', 'Keywords', 'Text', 'Summary'])\n",
        "\n",
        "# Initialize your model and tokenizer\n",
        "model, tokenizer = get_models_and_tokenizers()\n",
        "\n",
        "## ............................................... ##\n",
        "with st.container():\n",
        "    # Fetch news and process articles\n",
        "    if st.button('Fetch and Process News'):\n",
        "        # Your news retrieval code\n",
        "        google_news = GNews()\n",
        "        google_news.period = period  # News from last 7 days\n",
        "        google_news.max_results = max_results # number of responses across a keyword\n",
        "        google_news.country = country  # News from a specific country\n",
        "        google_news.language = language  # News in a specific language\n",
        "        #google_news.exclude_websites = ['yahoo.com', 'cnn.com']  # Exclude news from specific website i.e Yahoo.com and CNN.com\n",
        "        google_news.start_date = (start_date.year, start_date.month, start_date.day) # Search from 1st Jan 2023\n",
        "        google_news.end_date = (end_date.year, end_date.month, end_date.day) # Search until 1st Dec 2023\n",
        "\n",
        "        news = google_news.get_news(search_term)\n",
        "\n",
        "        ## ............................................... ##,\n",
        "        # Progress bar for fetching and processing news\n",
        "        progress_bar = st.progress(0)\n",
        "        total_news = len(news)\n",
        "\n",
        "        # Your news retrieval code (assuming 'news' is a list of article URLs)\n",
        "        #for x in news:\n",
        "        for idx, x in enumerate(news):\n",
        "            result = process_article(x['url'], _config=config)\n",
        "            if result is not None:\n",
        "                publish_date, language, url, source_url, title, authors, keywords, text, summary = result\n",
        "\n",
        "                # Insert to dataframe\n",
        "                temp_df = pd.DataFrame({'Publish_Date': [publish_date], 'Language': [language], 'URL': [url], 'Source_Url': [source_url], 'Title': [title], 'Authors': [authors], 'Keywords': [keywords],\n",
        "                                      'Text': [text], 'Summary': [summary]})\n",
        "                df = pd.concat([df, temp_df], ignore_index=True)\n",
        "\n",
        "                # Convert 'Publish_Date' to DatetimeIndex\n",
        "                df['Publish_Date'] = pd.to_datetime(df['Publish_Date'])\n",
        "\n",
        "            # Update the progress bar\n",
        "            progress = (idx + 1) / total_news\n",
        "            progress_bar.progress(progress)\n",
        "\n",
        "        # Conditionally apply translation function to the 'Translation' column\n",
        "        if include_translation:\n",
        "            df['Translation'] = df.apply(lambda row: translate_text((row['Title'] + ' | ' + row['Summary']), source=row['Language'], target='en'), axis=1)\n",
        "\n",
        "            # Preprocessing Data\n",
        "            df = preprocessing_data(df)\n",
        "\n",
        "        # Conditionally apply sentiment analysis function to the 'Translation' column\n",
        "        if include_sentiment_analysis:\n",
        "            df[['Fake Check', 'Sentiment Distilbert']] = df['Translation'].apply(lambda text: pd.Series(analyze_sentiment_distilbert(text, model, tokenizer)))\n",
        "\n",
        "\n",
        "        # Conditionally apply VADER sentiment analysis to the 'Translation' column\n",
        "        if include_sentiment_vader:\n",
        "            df['Sentiment VADER'] = df['Translation'].apply(analyze_sentiment_vader)\n",
        "\n",
        "        # Conditionally apply TextBlob sentiment analysis to the 'Translation' column\n",
        "        if include_sentiment_textblob:\n",
        "            df['Sentiment TextBlob'] = df['Translation'].apply(analyze_sentiment_textblob)\n",
        "\n",
        "        # Set data_processed to True when the data has been successfully processed\n",
        "        data_processed = True\n",
        "\n",
        "    ## ............................................... ##\n",
        "    # Add a button to download the data as a CSV file\n",
        "    if data_processed:\n",
        "        st.markdown(\"### Download Processed Data as CSV\")\n",
        "        st.write(\"Click the button below to download the processed data as a CSV file.\")\n",
        "\n",
        "        # Create a downloadable link\n",
        "        csv_data = df.to_csv(index=False).encode()\n",
        "        st.download_button(\n",
        "            label=\"Download CSV\",\n",
        "            data=csv_data,\n",
        "            file_name=\"processed_data.csv\",\n",
        "        )\n",
        "\n",
        "    ## ............................................... ##\n",
        "    with st.expander(\"See for Table\"):\n",
        "        # Display processed data\n",
        "        if data_processed:\n",
        "            AgGrid(df, height=400)\n",
        "\n",
        "    ## ............................................... ##\n",
        "    # Display processed data\n",
        "    with st.expander(\"See for Exploratory Data Analysis\"):\n",
        "        if data_processed:\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                ## ............................................... ##\n",
        "                # Create a DataFrame to count the number of tweets by Fake Check\n",
        "                FakeCheck_counts = df['Fake Check'].value_counts().reset_index()\n",
        "                FakeCheck_counts.columns = ['Fake Check', 'News Count']\n",
        "                fig = px.bar(FakeCheck_counts, x='Fake Check', y='News Count', text='News Count', title='Total News by Fake Check')\n",
        "                st.plotly_chart(fig, use_container_width=True, use_container_height=True, width=700, height=400)\n",
        "\n",
        "                ## ............................................... ##\n",
        "                # Create wordcloud\n",
        "                try:\n",
        "                    st.write('WordCloud for News')\n",
        "                    create_wordcloud(df)\n",
        "                except Exception as e:\n",
        "                    logging.error(f\" Column Translation Not Available : {str(e)}\")\n",
        "\n",
        "                ## ............................................... ##\n",
        "\n",
        "            with col2:\n",
        "                ## ............................................... ##\n",
        "                # Create a DataFrame to count the number of News by language\n",
        "                language_counts = df['Language'].value_counts().reset_index()\n",
        "                language_counts.columns = ['Language', 'News Count']\n",
        "                fig = px.bar(language_counts, x='Language', y='News Count', text='News Count', title='Total News by Language')\n",
        "                st.plotly_chart(fig, use_container_width=True, use_container_height=True, width=700, height=400)\n",
        "\n",
        "                ## ............................................... ##\n",
        "                # Group by Sentiment columns and get the count\n",
        "                try:\n",
        "                    sentiment_counts = df[['Sentiment Distilbert', 'Sentiment VADER', 'Sentiment TextBlob']].apply(lambda x: x.value_counts()).T\n",
        "                    sentiment_counts = sentiment_counts.reset_index()\n",
        "                    sentiment_counts = pd.melt(sentiment_counts, id_vars='index', var_name='Sentiment', value_name='Count')\n",
        "                    fig = px.bar(sentiment_counts, x='Sentiment', y='Count', color='index', barmode='group', title='Total News per Sentiment')\n",
        "                    st.plotly_chart(fig, use_container_width=True, use_container_height=True, width=700, height=400)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\" Column Sentiment Not Available : {str(e)}\")\n",
        "\n",
        "                ## ............................................... ##\n",
        "\n",
        "    with st.expander(\"See for Analysis with ydata-profiling\"):\n",
        "        ## ............................................... ##\n",
        "        # Display processed data\n",
        "        if data_processed:\n",
        "            pr = ProfileReport(df)\n",
        "            st_profile_report(pr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### requirements.txt"
      ],
      "metadata": {
        "id": "ofxv2bBWHMdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "beautifulsoup4==4.9.3\n",
        "chardet==5.2.0\n",
        "charset-normalizer==2.0.12\n",
        "deep-translator==1.11.4\n",
        "gnews==0.3.1\n",
        "langdetect==1.0.9\n",
        "newspaper3k==0.2.8\n",
        "nltk==3.8.1\n",
        "plotly==5.15.0\n",
        "requests==2.26.0\n",
        "sacremoses==0.0.53\n",
        "streamlit==1.27.0\n",
        "streamlit-aggrid==0.3.4.post3\n",
        "streamlit-pandas-profiling==0.1.3\n",
        "textblob==0.17.1\n",
        "torch==2.0.1\n",
        "transformers==4.34.0\n",
        "urllib3==1.26.18\n",
        "vaderSentiment==3.3.2\n",
        "ydata-profiling==4.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTqVp0d7GhFN",
        "outputId": "001b99b6-3db4-42ea-ff1c-cbfe50bf707b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "ydata-profiling\n",
        "streamlit\n",
        "streamlit-aggrid\n",
        "streamlit-pandas-profiling\n",
        "deep-translator\n",
        "gnews\n",
        "langdetect\n",
        "newspaper3k\n",
        "nltk\n",
        "plotly\n",
        "sacremoses\n",
        "textblob\n",
        "torch\n",
        "transformers\n",
        "vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeMv9_MsMbqt",
        "outputId": "5a7893ea-e7d7-4293-ba08-638d465a88b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_-RJKzBGqyk",
        "outputId": "2a02c2fc-a67b-4dc0-d3fa-68f07da2551b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/880.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/880.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/880.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/880.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m593.9/880.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m870.4/880.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m827.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list|grep streamlit"
      ],
      "metadata": {
        "id": "nd73TbwgqRod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ad452a-e687-4a0b-83f2-96ec75dbd390"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "streamlit                        1.27.0\n",
            "streamlit-aggrid                 0.3.4.post3\n",
            "streamlit-pandas-profiling       0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running"
      ],
      "metadata": {
        "id": "dvnbSPn62LMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8000 &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "LeXDV4yPc4rj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get upgrade && apt-get update"
      ],
      "metadata": {
        "id": "LE31sa2tZ1ni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fb6e3a-9f01-4325-fcf4-2fbb69ae2fff"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2\n",
            "0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 3,626 B in 4s (885 B/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel@latest"
      ],
      "metadata": {
        "id": "CrDjXpcJa2Wg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb5403c-387e-412d-a4ef-0fec6ba5e787"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 1.988s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com;  cat /content/logs.txt &"
      ],
      "metadata": {
        "id": "XoBmm9fzdxHQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4151aa3-b584-4f6e-afc9-7d6795952f8d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.69.241.40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8000"
      ],
      "metadata": {
        "id": "4DvoU_8TdEDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffd11fa-4482-4b71-f432-8e7dc1a2f694"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.829s\n",
            "your url is: https://mighty-baths-carry.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}